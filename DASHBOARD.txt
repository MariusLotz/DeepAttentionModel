Feature2LBinaryClassifier: 0.8136363636363636
SimpleBinaryClassifier: 0.8340909090909091
RawSimpleBinaryClassifier: 0.8242424242424242

Feature2LBinaryClassifier: 0.7727272727272727
SimpleBinaryClassifier: 0.8340909090909091
RawSimpleBinaryClassifier: 0.8242424242424242

Epoch [1/100], Loss: 0.6946
Epoch [2/100], Loss: 0.5546
Epoch [3/100], Loss: 0.7508
Epoch [4/100], Loss: 0.4936
Epoch [5/100], Loss: 0.4094
Epoch [6/100], Loss: 0.3975
Epoch [7/100], Loss: 0.3190
Epoch [8/100], Loss: 0.3404
Epoch [9/100], Loss: 0.2782
Epoch [10/100], Loss: 0.3063
Epoch [11/100], Loss: 0.2621
Epoch [12/100], Loss: 0.1422
Epoch [13/100], Loss: 0.2679
Epoch [14/100], Loss: 0.3327
Epoch [15/100], Loss: 0.3553
Epoch [16/100], Loss: 0.2087
Epoch [17/100], Loss: 0.2595
Epoch [18/100], Loss: 0.2384
Epoch [19/100], Loss: 0.1036
Epoch [20/100], Loss: 0.3325
Epoch [21/100], Loss: 0.0548
Epoch [22/100], Loss: 0.1500
Epoch [23/100], Loss: 0.1525
Epoch [24/100], Loss: 0.1366
Epoch [25/100], Loss: 0.0809
Epoch [26/100], Loss: 0.0996
Epoch [27/100], Loss: 0.0315
Epoch [28/100], Loss: 0.0073
Epoch [29/100], Loss: 0.0527
Epoch [30/100], Loss: 0.0319
Epoch [31/100], Loss: 0.0252
Epoch [32/100], Loss: 0.0669
Epoch [33/100], Loss: 0.0902
Epoch [34/100], Loss: 0.0513
Epoch [35/100], Loss: 0.0923
Epoch [36/100], Loss: 0.0942
Epoch [37/100], Loss: 0.1154
Epoch [38/100], Loss: 0.1538
Epoch [39/100], Loss: 0.3543
Epoch [40/100], Loss: 0.2097
Epoch [41/100], Loss: 0.0423
Epoch [42/100], Loss: 0.0466
Epoch [43/100], Loss: 0.1072
Epoch [44/100], Loss: 0.0249
Epoch [45/100], Loss: 0.0563
Epoch [46/100], Loss: 0.0405
Epoch [47/100], Loss: 0.1173
Epoch [48/100], Loss: 0.0469
Epoch [49/100], Loss: 0.0198
Epoch [50/100], Loss: 0.0242
Epoch [51/100], Loss: 0.0031
Epoch [52/100], Loss: 0.0041
Epoch [53/100], Loss: 0.0019
Epoch [54/100], Loss: 0.0043
Epoch [55/100], Loss: 0.0019
Epoch [56/100], Loss: 0.0053
Epoch [57/100], Loss: 0.0017
Epoch [58/100], Loss: 0.0062
Epoch [59/100], Loss: 0.0056
Epoch [60/100], Loss: 0.0025
Epoch [61/100], Loss: 0.0038
Epoch [62/100], Loss: 0.0025
Epoch [63/100], Loss: 0.0033
Epoch [64/100], Loss: 0.0027
Epoch [65/100], Loss: 0.0015
Epoch [66/100], Loss: 0.0021
Epoch [67/100], Loss: 0.0017
Epoch [68/100], Loss: 0.0029
Epoch [69/100], Loss: 0.0007
Epoch [70/100], Loss: 0.0025
Epoch [71/100], Loss: 0.0031
Epoch [72/100], Loss: 0.0006
Epoch [73/100], Loss: 0.0028
Epoch [74/100], Loss: 0.0006
Epoch [75/100], Loss: 0.0023
Epoch [76/100], Loss: 0.0023
Epoch [77/100], Loss: 0.0011
Epoch [78/100], Loss: 0.0010
Epoch [79/100], Loss: 0.0017
Epoch [80/100], Loss: 0.0010
Epoch [81/100], Loss: 0.0015
Epoch [82/100], Loss: 0.0004
Epoch [83/100], Loss: 0.0015
Epoch [84/100], Loss: 0.0009
Epoch [85/100], Loss: 0.0007
Epoch [86/100], Loss: 0.0014
Epoch [87/100], Loss: 0.0006
Epoch [88/100], Loss: 0.0020
Epoch [89/100], Loss: 0.0006
Epoch [90/100], Loss: 0.0005
Epoch [91/100], Loss: 0.0008
Epoch [92/100], Loss: 0.0005
Epoch [93/100], Loss: 0.0006
Epoch [94/100], Loss: 0.0004
Epoch [95/100], Loss: 0.0005
Epoch [96/100], Loss: 0.0007
Epoch [97/100], Loss: 0.0002
Epoch [98/100], Loss: 0.0006
Epoch [99/100], Loss: 0.0009
Epoch [100/100], Loss: 0.0005

import torch
import torch.nn as nn
import torch.nn.functional as F

class CustomDynamicLinearLayer(nn.Module):
    def __init__(self):
        super(CustomDynamicLinearLayer, self).__init__()

        # Initialize parameters as None, to be set during the first forward pass
        self.weight = None
        self.bias = None

    def initialize_layer(self, input_size):
        # Use input-dependent initialization
        self.weight = nn.Parameter(torch.randn(self.output_size, input_size))
        self.bias = nn.Parameter(torch.zeros(self.output_size))

    def forward(self, x):
        # If parameters are not initialized, set them based on the first input
        if self.weight is None or self.bias is None:
            self.output_size = x.size(1)  # Assuming input size is the second dimension
            self.initialize_layer(x.size(1))

        return F.linear(x, self.weight, self.bias)

# Example usage:
# Instantiate the custom dynamic linear layer
custom_dynamic_linear_layer = CustomDynamicLinearLayer()

# Create input tensor with varying input size
input_tensor1 = torch.randn(3, 5)  # Input size: 5
input_tensor2 = torch.randn(3, 8)  # Input size: 8

# Apply the custom dynamic linear layer
output1 = custom_dynamic_linear_layer(input_tensor1)
output2 = custom_dynamic_linear_layer(input_tensor2)

print("Custom Dynamic Linear Layer Output 1:", output1)
print("Custom Dynamic Linear Layer Output 2:", output2)
